diff --git a/reproduce_concat_issue.py b/reproduce_concat_issue.py
new file mode 100644
index 00000000..7a1aed98
--- /dev/null
+++ b/reproduce_concat_issue.py
@@ -0,0 +1,50 @@
+import xarray as xr
+import numpy as np
+
+# Create two datasets with different variables
+ds1 = xr.Dataset(
+    {
+        'temp': (['time', 'lat'], np.random.rand(2, 3)),
+        'precip': (['time', 'lat'], np.random.rand(2, 3))
+    },
+    coords={
+        'time': [1, 2],
+        'lat': [10, 20, 30]
+    }
+)
+
+ds2 = xr.Dataset(
+    {
+        'temp': (['time', 'lat'], np.random.rand(2, 3)),
+        'humidity': (['time', 'lat'], np.random.rand(2, 3))  # Different variable from ds1
+    },
+    coords={
+        'time': [3, 4],
+        'lat': [10, 20, 30]
+    }
+)
+
+print("Dataset 1:")
+print(ds1)
+print("\nDataset 2:")
+print(ds2)
+
+# Try with ignore_missing_vars=False (default behavior)
+try:
+    print("\nTrying to concatenate datasets with ignore_missing_vars=False...")
+    ds_concat = xr.concat([ds1, ds2], dim='time')
+    print("\nConcatenated result:")
+    print(ds_concat)
+except Exception as e:
+    print("\nError occurred:")
+    print(e)
+
+# Try with ignore_missing_vars=True (new behavior)
+try:
+    print("\nTrying to concatenate datasets with ignore_missing_vars=True...")
+    ds_concat = xr.concat([ds1, ds2], dim='time', ignore_missing_vars=True)
+    print("\nConcatenated result:")
+    print(ds_concat)
+except Exception as e:
+    print("\nError occurred:")
+    print(e)
\ No newline at end of file
diff --git a/test_edge_cases.py b/test_edge_cases.py
new file mode 100644
index 00000000..99ac2ef3
--- /dev/null
+++ b/test_edge_cases.py
@@ -0,0 +1,69 @@
+import xarray as xr
+import numpy as np
+
+# Create three datasets with different variables
+ds1 = xr.Dataset(
+    {
+        'temp': (['time', 'lat'], np.random.rand(2, 3)),
+        'precip': (['time', 'lat'], np.random.rand(2, 3))
+    },
+    coords={
+        'time': [1, 2],
+        'lat': [10, 20, 30]
+    }
+)
+
+ds2 = xr.Dataset(
+    {
+        'temp': (['time', 'lat'], np.random.rand(2, 3)),
+        'humidity': (['time', 'lat'], np.random.rand(2, 3))
+    },
+    coords={
+        'time': [3, 4],
+        'lat': [10, 20, 30]
+    }
+)
+
+ds3 = xr.Dataset(
+    {
+        'temp': (['time', 'lat'], np.random.rand(2, 3)),
+        'wind': (['time', 'lat'], np.random.rand(2, 3))
+    },
+    coords={
+        'time': [5, 6],
+        'lat': [10, 20, 30]
+    }
+)
+
+print("Dataset 1:")
+print(ds1)
+print("\nDataset 2:")
+print(ds2)
+print("\nDataset 3:")
+print(ds3)
+
+# Try concatenating all three datasets
+try:
+    print("\nTrying to concatenate all datasets with ignore_missing_vars=True...")
+    ds_concat = xr.concat([ds1, ds2, ds3], dim='time', ignore_missing_vars=True)
+    print("\nConcatenated result:")
+    print(ds_concat)
+except Exception as e:
+    print("\nError occurred:")
+    print(e)
+
+# Verify that the concatenated dataset has the correct dimensions
+print("\nVerifying dimensions...")
+print("Time dimension length:", len(ds_concat.time))  # Should be 6
+print("Lat dimension length:", len(ds_concat.lat))   # Should be 3
+
+# Verify that missing variables are filled with NaN in the right places
+print("\nVerifying NaN patterns...")
+print("precip NaN check (should be False, False, True, True, True, True):")
+print(np.isnan(ds_concat.precip).all(dim='lat').values)
+
+print("\nhumidity NaN check (should be True, True, False, False, True, True):")
+print(np.isnan(ds_concat.humidity).all(dim='lat').values)
+
+print("\nwind NaN check (should be True, True, True, True, False, False):")
+print(np.isnan(ds_concat.wind).all(dim='lat').values)
\ No newline at end of file
diff --git a/xarray/core/concat.py b/xarray/core/concat.py
index ecae2566..df71260f 100644
--- a/xarray/core/concat.py
+++ b/xarray/core/concat.py
@@ -1,5 +1,6 @@
 import pandas as pd
 
+import numpy as np
 from . import dtypes, utils
 from .alignment import align
 from .merge import _VALID_COMPAT, unique_variable
@@ -16,6 +17,7 @@ def concat(
     positions=None,
     fill_value=dtypes.NA,
     join="outer",
+    ignore_missing_vars=False,
 ):
     """Concatenate xarray objects along a new or existing dimension.
 
@@ -90,6 +92,10 @@ def concat(
         - 'override': if indexes are of same size, rewrite indexes to be
           those of the first object with that dimension. Indexes for the same
           dimension must have the same size in all objects.
+    ignore_missing_vars : bool, optional
+        If True, variables that are not present in all datasets will be filled
+        with NaN values in the datasets where they are missing. If False (default),
+        raises ValueError if variables are not present in all datasets.
 
     indexers, mode, concat_over : deprecated
 
@@ -128,7 +134,7 @@ def concat(
             "can only concatenate xarray Dataset and DataArray "
             "objects, got %s" % type(first_obj)
         )
-    return f(objs, dim, data_vars, coords, compat, positions, fill_value, join)
+    return f(objs, dim, data_vars, coords, compat, positions, fill_value, join, ignore_missing_vars)
 
 
 def _calc_concat_dim_coord(dim):
@@ -269,6 +275,7 @@ def _dataset_concat(
     positions,
     fill_value=dtypes.NA,
     join="outer",
+    ignore_missing_vars=False,
 ):
     """
     Concatenate a sequence of datasets along a new or existing dimension
@@ -354,14 +361,45 @@ def _dataset_concat(
                 var = var.set_dims(common_dims, common_shape)
             yield var
 
+    # Get all unique variable names across all datasets
+    all_vars = set()
+    for ds in datasets:
+        all_vars.update(ds.variables)
+    
+    # Remove dimension names from variables to avoid conflicts
+    all_vars = all_vars - set(dims_sizes.keys())
+    
     # stack up each variable to fill-out the dataset (in order)
-    # n.b. this loop preserves variable order, needed for groupby.
-    for k in datasets[0].variables:
-        if k in concat_over:
-            try:
-                vars = ensure_common_dims([ds.variables[k] for ds in datasets])
-            except KeyError:
-                raise ValueError("%r is not present in all datasets." % k)
+    # n.b. this loop preserves variable order for variables in the first dataset
+    for k in list(datasets[0].variables) + sorted(all_vars - set(datasets[0].variables)):
+        if k in dims_sizes:
+            continue  # Skip dimension variables
+        if k in concat_over or (ignore_missing_vars and any(k in ds.variables for ds in datasets)):
+            if ignore_missing_vars:
+                # Find the first dataset that has this variable to use as template
+                template_ds = next(ds for ds in datasets if k in ds.variables)
+                template_var = template_ds.variables[k]
+                
+                # Create a list of variables, using NaN for missing ones
+                vars_list = []
+                for ds in datasets:
+                    if k in ds.variables:
+                        vars_list.append(ds.variables[k])
+                    else:
+                        # Create a variable filled with NaN values matching the template
+                        shape = list(template_var.shape)
+                        # Replace the concat dimension size with the current dataset's size
+                        dim_idx = template_var.dims.index(dim) if dim in template_var.dims else 0
+                        shape[dim_idx] = ds.dims.get(dim, 1)
+                        nan_data = np.full(shape, np.nan, dtype=template_var.dtype)
+                        nan_var = Variable(template_var.dims, nan_data)
+                        vars_list.append(nan_var)
+                vars = ensure_common_dims(vars_list)
+            else:
+                try:
+                    vars = ensure_common_dims([ds.variables[k] for ds in datasets])
+                except KeyError:
+                    raise ValueError("%r is not present in all datasets." % k)
             combined = concat_vars(vars, dim, positions)
             assert isinstance(combined, Variable)
             result_vars[k] = combined
