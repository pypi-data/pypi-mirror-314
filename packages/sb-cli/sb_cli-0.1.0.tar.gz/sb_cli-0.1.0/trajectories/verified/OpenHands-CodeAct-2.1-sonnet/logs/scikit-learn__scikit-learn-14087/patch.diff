diff --git a/reproduce_error.py b/reproduce_error.py
new file mode 100644
index 000000000..508f4f9e8
--- /dev/null
+++ b/reproduce_error.py
@@ -0,0 +1,15 @@
+import sys
+import numpy as np
+from sklearn.linear_model import LogisticRegressionCV
+
+np.random.seed(29)
+X = np.random.normal(size=(1000, 3))
+beta = np.random.normal(size=3)
+intercept = np.random.normal(size=None)
+y = np.sign(intercept + X @ beta)
+
+LogisticRegressionCV(
+    cv=5,
+    solver='saga',  # same error with 'liblinear'
+    tol=1e-2,
+    refit=False).fit(X, y)
\ No newline at end of file
diff --git a/sklearn/linear_model/logistic.py b/sklearn/linear_model/logistic.py
index 1999f1dae..8db007798 100644
--- a/sklearn/linear_model/logistic.py
+++ b/sklearn/linear_model/logistic.py
@@ -2170,7 +2170,7 @@ class LogisticRegressionCV(LogisticRegression, BaseEstimator,
                 # Take the best scores across every fold and the average of
                 # all coefficients corresponding to the best scores.
                 best_indices = np.argmax(scores, axis=1)
-                if self.multi_class == 'ovr':
+                if self.multi_class == 'ovr' or len(self.classes_) == 2:
                     w = np.mean([coefs_paths[i, best_indices[i], :]
                                  for i in range(len(folds))], axis=0)
                 else:
@@ -2180,8 +2180,16 @@ class LogisticRegressionCV(LogisticRegression, BaseEstimator,
                 best_indices_C = best_indices % len(self.Cs_)
                 self.C_.append(np.mean(self.Cs_[best_indices_C]))
 
-                best_indices_l1 = best_indices // len(self.Cs_)
-                self.l1_ratio_.append(np.mean(l1_ratios_[best_indices_l1]))
+                if self.penalty == 'elasticnet':
+                    best_indices_l1 = best_indices // len(self.Cs_)
+                    # Handle case where best_indices_l1 is array
+                    if hasattr(best_indices_l1, '__len__'):
+                        l1_ratios_best = [l1_ratios_[i] for i in best_indices_l1]
+                        self.l1_ratio_.append(np.mean(l1_ratios_best))
+                    else:
+                        self.l1_ratio_.append(l1_ratios_[best_indices_l1])
+                else:
+                    self.l1_ratio_.append(None)
 
             if multi_class == 'multinomial':
                 self.C_ = np.tile(self.C_, n_classes)
diff --git a/test_elasticnet.py b/test_elasticnet.py
new file mode 100644
index 000000000..e7077c6ba
--- /dev/null
+++ b/test_elasticnet.py
@@ -0,0 +1,17 @@
+import numpy as np
+from sklearn.linear_model import LogisticRegressionCV
+
+np.random.seed(29)
+X = np.random.normal(size=(1000, 3))
+beta = np.random.normal(size=3)
+intercept = np.random.normal(size=None)
+y = np.sign(intercept + X @ beta)
+
+# Test elasticnet with refit=False
+LogisticRegressionCV(
+    cv=5,
+    solver='saga',
+    penalty='elasticnet',
+    l1_ratios=[0.5],
+    tol=1e-2,
+    refit=False).fit(X, y)
\ No newline at end of file
diff --git a/test_multinomial.py b/test_multinomial.py
new file mode 100644
index 000000000..f028f6458
--- /dev/null
+++ b/test_multinomial.py
@@ -0,0 +1,15 @@
+import numpy as np
+from sklearn.linear_model import LogisticRegressionCV
+
+# Generate 3-class data
+np.random.seed(29)
+X = np.random.normal(size=(1000, 3))
+y = np.random.randint(0, 3, size=1000)
+
+# Test multinomial with refit=False
+LogisticRegressionCV(
+    cv=5,
+    solver='saga',
+    tol=1e-2,
+    multi_class='multinomial',
+    refit=False).fit(X, y)
\ No newline at end of file
