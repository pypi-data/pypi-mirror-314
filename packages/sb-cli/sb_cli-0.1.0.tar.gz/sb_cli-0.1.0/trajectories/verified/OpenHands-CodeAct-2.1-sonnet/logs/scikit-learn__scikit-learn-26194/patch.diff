diff --git a/reproduce_roc_curve.py b/reproduce_roc_curve.py
new file mode 100644
index 000000000..f8c4fad7c
--- /dev/null
+++ b/reproduce_roc_curve.py
@@ -0,0 +1,25 @@
+import numpy as np
+from sklearn.metrics import roc_curve
+
+# Set random seed for reproducibility
+rng = np.random.RandomState(42)
+
+# Generate random binary labels and probability scores
+y_true = rng.randint(0, 2, size=10)
+y_score = rng.rand(10)  # Random probabilities between 0 and 1
+
+print("y_true:", y_true)
+print("y_score:", y_score)
+
+# Calculate ROC curve
+fpr, tpr, thresholds = roc_curve(y_true, y_score)
+
+print("\nThresholds:", thresholds)
+print("Max threshold:", thresholds.max())
+print("Min threshold:", thresholds.min())
+
+# Check if all thresholds are in [0,1] range
+valid_thresholds = np.logical_and(thresholds <= 1, thresholds >= 0).all()
+print("\nAll thresholds in [0,1] range?", valid_thresholds)
+if not valid_thresholds:
+    print("Invalid thresholds:", thresholds[~np.logical_and(thresholds <= 1, thresholds >= 0)])
\ No newline at end of file
diff --git a/sklearn/metrics/_ranking.py b/sklearn/metrics/_ranking.py
index 0cdead923..7e3f2e1ee 100644
--- a/sklearn/metrics/_ranking.py
+++ b/sklearn/metrics/_ranking.py
@@ -1083,7 +1083,16 @@ def roc_curve(
     # to make sure that the curve starts at (0, 0)
     tps = np.r_[0, tps]
     fps = np.r_[0, fps]
-    thresholds = np.r_[thresholds[0] + 1, thresholds]
+    # If y_score appears to be probabilities (max value <= 1), clip to 1
+    # Otherwise add 1 to max_score
+    max_score = thresholds[0]
+    if max_score <= 1:
+        # For probabilities, the extra threshold should be exactly 1
+        # This handles edge cases where max_score is 1 or very close to 1
+        extra_threshold = 1.0
+    else:
+        extra_threshold = max_score + 1
+    thresholds = np.r_[extra_threshold, thresholds]
 
     if fps[-1] <= 0:
         warnings.warn(
diff --git a/test_edge_cases.py b/test_edge_cases.py
new file mode 100644
index 000000000..83bb6f9bd
--- /dev/null
+++ b/test_edge_cases.py
@@ -0,0 +1,38 @@
+import numpy as np
+from sklearn.metrics import roc_curve
+
+def test_case(y_true, y_score, name):
+    print(f"\nTest case: {name}")
+    print("y_true:", y_true)
+    print("y_score:", y_score)
+    
+    fpr, tpr, thresholds = roc_curve(y_true, y_score)
+    
+    print("Thresholds:", thresholds)
+    print("Max threshold:", thresholds.max())
+    print("Min threshold:", thresholds.min())
+    
+    valid_thresholds = np.logical_and(thresholds <= 1, thresholds >= 0).all()
+    print("All thresholds in [0,1] range?", valid_thresholds)
+    if not valid_thresholds:
+        print("Invalid thresholds:", thresholds[~np.logical_and(thresholds <= 1, thresholds >= 0)])
+
+# Test case 1: All probabilities are 1 or 0
+y_true = np.array([0, 1, 0, 1])
+y_score = np.array([0, 1, 0, 1])
+test_case(y_true, y_score, "Binary probabilities (0/1)")
+
+# Test case 2: All probabilities are exactly 1
+y_true = np.array([0, 1, 0, 1])
+y_score = np.array([1, 1, 1, 1])
+test_case(y_true, y_score, "All probabilities = 1")
+
+# Test case 3: All probabilities are exactly 0
+y_true = np.array([0, 1, 0, 1])
+y_score = np.array([0, 0, 0, 0])
+test_case(y_true, y_score, "All probabilities = 0")
+
+# Test case 4: Decision function scores (not probabilities)
+y_true = np.array([0, 1, 0, 1])
+y_score = np.array([-2.5, 1.8, -1.2, 2.3])
+test_case(y_true, y_score, "Decision function scores")
\ No newline at end of file
