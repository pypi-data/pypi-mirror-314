"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .generateresponserequest_query_translation_strategy import (
    GenerateResponseRequestQueryTranslationStrategy,
    GenerateResponseRequestQueryTranslationStrategyTypedDict,
)
from .generateresponserequest_retrieval_strategy import (
    GenerateResponseRequestRetrievalStrategy,
    GenerateResponseRequestRetrievalStrategyTypedDict,
)
from .generateresponserequest_self_correction_strategy import (
    GenerateResponseRequestSelfCorrectionStrategy,
    GenerateResponseRequestSelfCorrectionStrategyTypedDict,
)
from duckyai.types import BaseModel
from typing import Dict, Optional
from typing_extensions import NotRequired, TypedDict


class WebappGenerateResponseRequestTypedDict(TypedDict):
    project_id: str
    r"""The project ID associated with the request."""
    max_completion_tokens: NotRequired[int]
    r"""The max number of completion tokens to generate"""
    index_name: NotRequired[str]
    r"""The name of the index to retrieve documents from"""
    retrieval_query: NotRequired[str]
    r"""The query used to retrieve relevant documents"""
    prompt: NotRequired[str]
    r"""If not using a prompt template, user can specify a prompt manually. Useful for testing."""
    prompt_template_id: NotRequired[str]
    r"""The ID of the prompt template to use"""
    variables: NotRequired[Dict[str, str]]
    r"""Variables to inject into the prompt"""
    evaluation_id: NotRequired[str]
    r"""The ID representing the evaluation to compare variant results in (optional)"""
    variant_name: NotRequired[str]
    r"""A name of the evaluation variant to use, allowing you to track performance across different variants (optional)"""
    retrieval_strategy: NotRequired[GenerateResponseRequestRetrievalStrategyTypedDict]
    r"""Configuration for document retrieval"""
    self_correction_strategy: NotRequired[
        GenerateResponseRequestSelfCorrectionStrategyTypedDict
    ]
    r"""Configuration for self-correction strategy on the generated result"""
    query_translation_strategy: NotRequired[
        GenerateResponseRequestQueryTranslationStrategyTypedDict
    ]
    r"""Configuration for query translation"""


class WebappGenerateResponseRequest(BaseModel):
    project_id: str
    r"""The project ID associated with the request."""

    max_completion_tokens: Optional[int] = None
    r"""The max number of completion tokens to generate"""

    index_name: Optional[str] = None
    r"""The name of the index to retrieve documents from"""

    retrieval_query: Optional[str] = None
    r"""The query used to retrieve relevant documents"""

    prompt: Optional[str] = None
    r"""If not using a prompt template, user can specify a prompt manually. Useful for testing."""

    prompt_template_id: Optional[str] = None
    r"""The ID of the prompt template to use"""

    variables: Optional[Dict[str, str]] = None
    r"""Variables to inject into the prompt"""

    evaluation_id: Optional[str] = None
    r"""The ID representing the evaluation to compare variant results in (optional)"""

    variant_name: Optional[str] = None
    r"""A name of the evaluation variant to use, allowing you to track performance across different variants (optional)"""

    retrieval_strategy: Optional[GenerateResponseRequestRetrievalStrategy] = None
    r"""Configuration for document retrieval"""

    self_correction_strategy: Optional[
        GenerateResponseRequestSelfCorrectionStrategy
    ] = None
    r"""Configuration for self-correction strategy on the generated result"""

    query_translation_strategy: Optional[
        GenerateResponseRequestQueryTranslationStrategy
    ] = None
    r"""Configuration for query translation"""
