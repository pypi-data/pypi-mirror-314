Metadata-Version: 2.1
Name: iqm-benchmarks
Version: 1.7
Summary: A package for implementation of Quantum Characterization, Verification and Validation (QCVV) techniques on IQM's hardware at gate level abstraction
Author-email: IQM Finland Oy <developers@meetiqm.com>, Aniket Rath <aniket.rath@meetiqm.com>, Jami Rönkkö <jami@meetiqm.com>, Pedro Figueroa Romero <pedro.romero@meetiqm.com>, Vicente Pina Canelles <vicente.pina@meetiqm.com>, Raphael Brieger <raphael.brieger@meetiqm.com>, Stefan Seegerer <stefan.seegerer@meetiqm.com>
Project-URL: Homepage, https://github.com/iqm-finland/iqm-benchmarks
Classifier: Development Status :: 4 - Beta
Classifier: Programming Language :: Python :: 3 :: Only
Classifier: Topic :: Scientific/Engineering :: Physics
Classifier: Intended Audience :: Science/Research
Requires-Python: >=3.11
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: lmfit>=1.2
Requires-Dist: matplotlib<4,>=3.6.3
Requires-Dist: more-itertools==10.1.0
Requires-Dist: mthree<2.7,>=2.6
Requires-Dist: networkx==3.3
Requires-Dist: numpy<2.0,>=1.25.2
Requires-Dist: qiskit>=1.0
Requires-Dist: qiskit-iqm>=15.1
Requires-Dist: scikit-optimize==0.10.2
Requires-Dist: tabulate==0.9.0
Requires-Dist: uncertainties==3.2.2
Requires-Dist: pycurl==7.45.3
Requires-Dist: xarray==2024.6.0
Requires-Dist: types-pycurl
Provides-Extra: develop
Requires-Dist: tox==4.16.0; extra == "develop"
Provides-Extra: examples
Requires-Dist: notebook==7.2.1; extra == "examples"
Provides-Extra: mgst
Requires-Dist: numpy<2.0; extra == "mgst"
Requires-Dist: cvxpy==1.5.3; extra == "mgst"
Requires-Dist: pygsti==0.9.12.3; extra == "mgst"
Requires-Dist: tqdm==4.66.5; extra == "mgst"
Requires-Dist: numba==0.60.0; extra == "mgst"
Provides-Extra: test
Requires-Dist: black==24.4.2; extra == "test"
Requires-Dist: isort==5.13.2; extra == "test"
Requires-Dist: mypy==1.10.1; extra == "test"
Requires-Dist: pylint==3.2.5; extra == "test"
Requires-Dist: pytest==7.4.4; extra == "test"
Requires-Dist: pytest-cov==4.1.0; extra == "test"
Requires-Dist: pytest-isort==3.1.0; extra == "test"
Requires-Dist: pytest-mypy==0.10.3; extra == "test"
Requires-Dist: pytest-pylint==0.21.0; extra == "test"

# IQM Benchmarks

The IQM Benchmarks is a suite of quantum characterization, verification, and validation (QCVV) tools for quantum computing. It is designed to be a comprehensive tool for benchmarking quantum hardware. The suite is designed to be modular, allowing users to easily add new benchmarks and customize existing ones. The suite is designed to be easy to use, with a simple API that allows users to run benchmarks with a single command.


Below is a list of the benchmarks currently available in the suite:
* Randomized Benchmarking: A suite of randomized benchmarking protocols for characterizing the performance of quantum gates (Clifford Randomized Benchmarking, Mirror Randomized Benchmarking, Interleaved Randomized Benchmarking).
* Quantum Volume: A benchmark for characterizing the performance of quantum computers.
* Q-Score: A benchmark that estimates the size of combinatorial optimization problems a given number of qubits can execute with meaningful results.
* GHZ State Benchmarking: A benchmark for characterizing the performance of multi-qubit entangled states.


The project is split into different benchmarks, all sharing the `Benchmark` class or the legacy `BenchmarkBase` class. Each individual benchmark takes as an argument their own `BenchmarkConfigurationBase` class. All the (legacy) benchmarks executed at once are wrapped by the `BenchmarkExperiment` class, which handles dependencies among the benchmarks, storing the results, producing the plots...


## Installation _(latest release)_

Usually it makes sense to use a new Conda environment (e.g. ``iqm-benchmarks``) to isolate your setup from the global Python installation. That way, you can play around without messing the rest of your system.

Start a terminal in your machine, and type

```
conda create -n iqm-benchmarks python=3.11.2
conda activate iqm-benchmarks
```

Then, you can install the latest release of the IQM Benchmarks by running:
```bash
$ pip install iqm-benchmarks
```

If you have already installed `iqm-benchmarks` and want to get the latest release you can add the --upgrade flag:

```bash
pip install iqm-benchmarks --upgrade
```

## Development mode _(latest changes: recommended)_

To install in development mode with all required dependencies, you can instead clone the [repository](https://www.github.com/iqm-finland/iqm-benchmarks) and from the project directory run

```bash
python -m pip install -e ".[develop,test]" --upgrade --upgrade-strategy=eager
```

To run the tests, you can use the following command:

```bash
tox -e test
```

## Characterize Physical Hardware

The IQM Benchmarks suite is designed to be used with real quantum hardware. To use the suite, you will need to have access to a quantum computer. The suite is designed to work with both IQM Resonance (IQM's quantum cloud service) and on-prem devices, but can be easily adapted to work with other quantum computing platforms.

To use the suite with IQM Resonance, you will need to set up an account and obtain an API token. You can then set the `IQM_TOKEN` environment variable to your API token. The suite will automatically use this token to authenticate with IQM Resonance.

```python
import os
os.environ["IQM_TOKEN"] = "your_token"
```

### Using a Jupyter notebook or Python script

You can easily set up one or more benchmarks by defining a configuration for them. For example, for Randomized, Interleaved and Mirror Benchmarking, or Quantum Volume:

```python
from iqm.benchmarks.randomized_benchmarking.interleaved_rb.interleaved_rb import InterleavedRBConfiguration
from iqm.benchmarks.randomized_benchmarking.mirror_rb.mirror_rb import MirrorRBConfiguration
from iqm.benchmarks.quantum_volume.quantum_volume import QuantumVolumeConfiguration

EXAMPLE_IRB = InterleavedRBConfiguration(
    qubits_array=[[3,4],[8,9]],
    sequence_lengths=[2**(m+1)-1 for m in range(7)],
    num_circuit_samples=30,
    shots=2**10,
    calset_id=None,
    parallel_execution=True,
    interleaved_gate = "iSwapGate",
    interleaved_gate_params = None,
    simultaneous_fit = ["amplitude", "offset"],
)

EXAMPLE_MRB = MirrorRBConfiguration(
    qubits_array=[[0,1],
                  [0,1,3,4],
                  [0,1,3,4,8,9],
                  [0,1,3,4,8,9,13,14],
                  [0,1,3,4,8,9,13,14,17,18]],
    depths_array=[[2**m for m in range(9)],
                  [2**m for m in range(8)],
                  [2**m for m in range(7)],
                  [2**m for m in range(6)],
                  [2**m for m in range(5)]],
    num_circuit_samples=10,
    num_pauli_samples=5,
    shots=2**8,
    two_qubit_gate_ensemble={"CZGate": 0.7, "iSwapGate": 0.3}, # {GATE: PROBABILITY}
    density_2q_gates=0.25,
    calset_id=None,
)

EXAMPLE_QV = QuantumVolumeConfiguration(
    num_circuits=500,
    shots=2**8,
    calset_id=None,
    num_sigmas=2,
    choose_qubits_routine="custom",
    custom_qubits_array=[[0,1,2,3], [0,1,3,4]],
    qiskit_optim_level=3,
    optimize_sqg=True,
    routing_method="sabre",
    physical_layout="fixed",
    max_gates_per_batch=60_000,
    rem=True,
    mit_shots=1_000,
)
```

In order to execute them, you must specify a backend.
* for IQM Resonance this can be given as a simple string, such as "garnet" (together with your IQM Token environment variable)
* and for an on-prem device and IQM Resonance this can be defined using the URL of the quantum computer.

Also, you need to reference the benchmark configuration you want to run:

```python
from iqm.benchmarks.randomized_benchmarking.mirror_rb.mirror_rb import *
# import os
# os.environ["IQM_TOKEN"] = "your_token"

backend = IQMProvider("https://example-station.qc.iqm.fi/cocos/").get_backend()

EXAMPLE_EXPERIMENT = MirrorRandomizedBenchmarking(backend, EXAMPLE_MRB)
EXAMPLE_EXPERIMENT.run()
```

Full examples on how to run benchmarks and analyze the results can be found in the `examples` folder.

### Scheduled benchmarks using a CI/CD Pipeline

This repository can be setup to perform a scheduled (weekly, daily...) benchmark from a Gitlab/Github pipeline, executed on a real device. An example configuration is given in the `scheduled_experiments` folder.
