# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

from typing import TYPE_CHECKING, Dict, List, Union, Optional
from typing_extensions import Literal, TypeAlias

from ...._models import BaseModel

__all__ = [
    "ChatCompletion",
    "Choice",
    "ChoiceChatCompletionChoice",
    "ChoiceChatCompletionChoiceMessage",
    "ChoiceChatCompletionChoiceMessageToolCall",
    "ChoiceChatCompletionChoiceMessageToolCallFunction",
    "ChoiceChatCompletionChoiceLogprobs",
    "ChoiceChatCompletionChoiceLogprobsContent",
    "ChoiceChatCompletionChoiceLogprobsContentTopLogprob",
    "Usage",
    "UsageUsage",
    "UsageUsageCompletionTokensDetails",
    "UsageUsagePromptTokensDetails",
    "UsageUsageStatistics",
    "UsageUsageStatisticsCompletionTokensDetails",
    "UsageUsageStatisticsCompletionTokensDetailsCompletionTokensDetailsWrapper",
    "UsageUsageStatisticsPromptTokensDetails",
    "UsageUsageStatisticsPromptTokensDetailsPromptTokensDetailsWrapper",
]


class ChoiceChatCompletionChoiceMessageToolCallFunction(BaseModel):
    name: str
    """The name of the function."""

    arguments: Optional[str] = None
    """
    The arguments to call the function with, as generated by the model in JSON
    format. Note that the model does not always generate valid JSON, and may
    hallucinate parameters not defined by your function schema. Validate the
    arguments in your code before calling your function.
    """

    type: Optional[str] = None
    """The type of the function."""


class ChoiceChatCompletionChoiceMessageToolCall(BaseModel):
    id: str
    """The ID of the tool call."""

    function: Optional[ChoiceChatCompletionChoiceMessageToolCallFunction] = None
    """The function that was called."""

    type: Optional[Literal["function"]] = None
    """The type of the tool call."""


class ChoiceChatCompletionChoiceMessage(BaseModel):
    content: Optional[str] = None
    """The content of the message."""

    refusal: Optional[str] = None
    """The refusal message generated by the model."""

    role: Optional[str] = None
    """
    The role of the author of this message.This field is only relevant if the
    completion is part of a multi-turn conversation.
    """

    tool_calls: Optional[List[ChoiceChatCompletionChoiceMessageToolCall]] = None
    """Tool calls generated by the model."""


class ChoiceChatCompletionChoiceLogprobsContentTopLogprob(BaseModel):
    token: str

    logprob: float

    bytes: Optional[List[int]] = None


class ChoiceChatCompletionChoiceLogprobsContent(BaseModel):
    token: str

    logprob: float

    top_logprobs: List[ChoiceChatCompletionChoiceLogprobsContentTopLogprob]

    bytes: Optional[List[int]] = None


class ChoiceChatCompletionChoiceLogprobs(BaseModel):
    content: Optional[List[ChoiceChatCompletionChoiceLogprobsContent]] = None
    """A list of message content tokens with log probability information."""


class ChoiceChatCompletionChoice(BaseModel):
    index: str
    """The index of the choice."""

    message: ChoiceChatCompletionChoiceMessage
    """The message generated by the model."""

    finish_reason: Optional[str] = None
    """The reason the completion stopped."""

    logprobs: Optional[ChoiceChatCompletionChoiceLogprobs] = None
    """Log probabilities of the tokens generated."""


Choice: TypeAlias = Union[Dict[str, object], ChoiceChatCompletionChoice]


class UsageUsageCompletionTokensDetails(BaseModel):
    accepted_prediction_tokens: Optional[int] = None

    audio_tokens: Optional[int] = None

    reasoning_tokens: Optional[int] = None

    rejected_prediction_tokens: Optional[int] = None

    if TYPE_CHECKING:
        # Stub to indicate that arbitrary properties are accepted.
        # To access properties that are not valid identifiers you can use `getattr`, e.g.
        # `getattr(obj, '$type')`
        def __getattr__(self, attr: str) -> object: ...


class UsageUsagePromptTokensDetails(BaseModel):
    audio_tokens: Optional[int] = None

    cached_tokens: Optional[int] = None

    if TYPE_CHECKING:
        # Stub to indicate that arbitrary properties are accepted.
        # To access properties that are not valid identifiers you can use `getattr`, e.g.
        # `getattr(obj, '$type')`
        def __getattr__(self, attr: str) -> object: ...


class UsageUsage(BaseModel):
    completion_tokens: int

    prompt_tokens: int

    total_tokens: int

    completion_tokens_details: Optional[UsageUsageCompletionTokensDetails] = None

    prompt_tokens_details: Optional[UsageUsagePromptTokensDetails] = None

    if TYPE_CHECKING:
        # Stub to indicate that arbitrary properties are accepted.
        # To access properties that are not valid identifiers you can use `getattr`, e.g.
        # `getattr(obj, '$type')`
        def __getattr__(self, attr: str) -> object: ...


class UsageUsageStatisticsCompletionTokensDetailsCompletionTokensDetailsWrapper(BaseModel):
    accepted_prediction_tokens: Optional[int] = None

    audio_tokens: Optional[int] = None

    reasoning_tokens: Optional[int] = None

    rejected_prediction_tokens: Optional[int] = None

    text_tokens: Optional[int] = None

    if TYPE_CHECKING:
        # Stub to indicate that arbitrary properties are accepted.
        # To access properties that are not valid identifiers you can use `getattr`, e.g.
        # `getattr(obj, '$type')`
        def __getattr__(self, attr: str) -> object: ...


UsageUsageStatisticsCompletionTokensDetails: TypeAlias = Union[
    UsageUsageStatisticsCompletionTokensDetailsCompletionTokensDetailsWrapper, object
]


class UsageUsageStatisticsPromptTokensDetailsPromptTokensDetailsWrapper(BaseModel):
    audio_tokens: Optional[int] = None

    cached_tokens: Optional[int] = None

    image_tokens: Optional[int] = None

    text_tokens: Optional[int] = None

    if TYPE_CHECKING:
        # Stub to indicate that arbitrary properties are accepted.
        # To access properties that are not valid identifiers you can use `getattr`, e.g.
        # `getattr(obj, '$type')`
        def __getattr__(self, attr: str) -> object: ...


UsageUsageStatisticsPromptTokensDetails: TypeAlias = Union[
    UsageUsageStatisticsPromptTokensDetailsPromptTokensDetailsWrapper, object
]


class UsageUsageStatistics(BaseModel):
    completion_tokens: Optional[int] = None
    """The number of tokens in the output completion."""

    completion_tokens_details: Optional[UsageUsageStatisticsCompletionTokensDetails] = None
    """Breakdown of tokens used in the prompt."""

    prompt_tokens: Optional[int] = None
    """The number of tokens in the generated completion."""

    prompt_tokens_details: Optional[UsageUsageStatisticsPromptTokensDetails] = None
    """Breakdown of tokens used in a completion."""

    total_tokens: Optional[int] = None
    """The total number of tokens in the prompt and completion."""


Usage: TypeAlias = Union[UsageUsage, UsageUsageStatistics]


class ChatCompletion(BaseModel):
    choices: List[Choice]

    id: Optional[str] = None
    """A unique identifier for the completion."""

    created: Optional[int] = None
    """The Unix timestamp (in seconds) of when the chat completion was created."""

    model: Optional[str] = None
    """The model used for completion."""

    object: Optional[str] = None
    """The object type, ex `text_completion` or `chat.completion`"""

    system_fingerprint: Optional[str] = None
    """This fingerprint represents the backend configuration that the model runs with."""

    usage: Optional[Usage] = None
    """Usage statistics."""
