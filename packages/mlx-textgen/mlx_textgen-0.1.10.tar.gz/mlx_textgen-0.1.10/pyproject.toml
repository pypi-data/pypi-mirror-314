[project]
name = "mlx-textgen"
description = "An OpenAI-compatible API LLM engine with smart prompt caching, batch processing, structured output with guided decoding, and function calling for all models using MLX."
dynamic = ["version"]

authors = [
    {name = "Nathan Tam", email = "nathan1295@gmail.com"},
]
requires-python = ">=3.9"
readme = "README.md"
license = {text = "MIT"}
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: MacOS :: MacOS X",
]
dependencies = [
    "mlx-lm>=0.19.1",
    "outlines>=0.1.1",
    "fastapi",
    "uvicorn"
]

[project.urls]
Homepage = "https://github.com/nath1295/MLX-Textgen"

[project.scripts]
"mlx_textgen.server" = "mlx_textgen.server:main"
"mlx_textgen.clear_cache" = "mlx_textgen.model_utils:clear_prompt_cache"
"mlx_textgen.create_config" = "mlx_textgen.create_config:main"

[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[tool.setuptools]
py-modules = []

[tool.setuptools.dynamic]
version = {attr = "mlx_textgen.__version__"}

[tool.setuptools.packages.find]
where = ["src"]